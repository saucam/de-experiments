{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0e2f9c9-7f87-42f2-8c21-3d5d55738a37",
   "metadata": {},
   "source": [
    "# Parquet For Data Processing\n",
    "\n",
    "This is a demo of parquet data format and its capabilities for big data processing.\n",
    "For this demo, we will be using pandas, sqlite, pyarrow and pyspark libraries to demonstrate the parquet capabilities.\n",
    "The dataset that we will use is an sqlite dump of [wikibooks](https://www.kaggle.com/datasets/dhruvildave/wikibooks-dataset) from kaggle. It contains 270K chapters of wikibooks in 12 languages, but we will concentrate on the English version. To access this dataset you need to setup kaggle account and download your [kaggle.json file before proceeding](https://www.kaggle.com/docs/api#authentication).\n",
    "\n",
    "- pandas is an open source library providing high-performance, easy-to-use data structures and data analysis tools in Python.\n",
    "- sqlite is an embedded SQL database engine, that uses the more traditional [B-Tree data-format](https://www.sqlite.org/fileformat2.html) for storage on disk \n",
    "- pyarrow is Python API of the [Apache Arrow](https://arrow.apache.org/) framework that defines an in-memory data representation and can read/write parquet, including conversion to pandas. There are alternatives like [fastparquet](https://pypi.org/project/fastparquet/) which can also be explored.\n",
    "- pyspark is a Python API to the [Apache Spark Engine](https://spark.apache.org/), interfaces Python commands with a Java/Scala execution core, and thereby gives Python programmers access to the Parquet format as parquet is natively supported in spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66d1e0fa-16bb-4f1c-a500-5dd450ad1b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cf05d87-68a6-4793-b77b-24259b4217cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import FileUpload\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f19b96-c71b-4fc9-ba27-109ad86dd19c",
   "metadata": {},
   "source": [
    "# Creating Parquet dataset from sqlite\n",
    "We will fetch the sqlite dataset and convert and store each table into parquet files. We can then compare the on-disk sizes to get an idea of how efficient parquet is. Note that dataset is about 1.8G so might take a while to download depending on your network speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf57c6-e38b-4bb8-8d8e-ca0a53547cc9",
   "metadata": {},
   "source": [
    "## Setting up kaggle token and downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a4754c-5371-4103-b4e9-0ca2c5167863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle.json file not found. Please upload the file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6593e82f759f427290a077d58a8cd8f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='application/json', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download your kaggle credentials file from kaggle and supply it here, only necessary if you have not yet setup your kaggle credentials\n",
    "# in the .kaggle folder in your home dir\n",
    "# Path to the .kaggle directory\n",
    "kaggle_dir = os.path.expanduser('~/.kaggle')\n",
    "kaggle_file_path = os.path.join(kaggle_dir, 'kaggle.json')\n",
    "\n",
    "# Function to check and prompt for file upload\n",
    "def check_and_prompt_for_upload():\n",
    "    if not os.path.isfile(kaggle_file_path):\n",
    "        print(\"kaggle.json file not found. Please upload the file.\")\n",
    "        upload = FileUpload(accept='application/json', multiple=False)\n",
    "        display(upload)\n",
    "        return upload\n",
    "    else:\n",
    "        print(\"kaggle.json file already exists in the '~/.kaggle' directory.\")\n",
    "        return None\n",
    "\n",
    "# Adjusted function to process the uploaded file based on the provided structure and set permissions\n",
    "def process_uploaded_file(upload_widget):\n",
    "    # Ensure the .kaggle directory exists\n",
    "    os.makedirs(kaggle_dir, exist_ok=True)\n",
    "    \n",
    "    if upload_widget:\n",
    "        # Assuming the first item in the tuple is the file info dictionary\n",
    "        file_info = upload_widget.value[0]  # Extract the file details from the tuple\n",
    "        \n",
    "        content = file_info['content']\n",
    "        with open(kaggle_file_path, 'wb') as f:\n",
    "            f.write(content)\n",
    "        print(f\"'{file_info['name']}' has been moved to '{kaggle_dir}'.\")\n",
    "\n",
    "        # Set file permissions to 600\n",
    "        os.chmod(kaggle_file_path, 0o600)\n",
    "        print(f\"Permissions for '{file_info['name']}' set to 600.\")\n",
    "\n",
    "upload = check_and_prompt_for_upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e2f457-f36f-4947-bef7-deb43e030d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'kaggle.json' has been moved to '/home/ydatta/.kaggle'.\n",
      "Permissions for 'kaggle.json' set to 600.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if upload.value:\n",
    "        process_uploaded_file(upload)\n",
    "except NameError:\n",
    "    print(\"Upload widget not displayed or file already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d1898b0-192b-4ced-8051-26afdf75e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d03e29-7d7e-48dc-81a7-f3f1eb523d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading wikibooks-dataset.zip to /home/ydatta/Workspace/de-experiments/data/parquet\n",
      "100%|██████████████████████████████████████| 1.82G/1.82G [11:32<00:00, 3.35MB/s]\n",
      "100%|██████████████████████████████████████| 1.82G/1.82G [11:32<00:00, 2.83MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d dhruvildave/wikibooks-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24ced0d8-f657-4408-98a4-f7bebf3ccb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "file_name = 'wikibooks-dataset.zip' #the file is your dataset exact name\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "  zip.extractall()\n",
    "  print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ca91e-bf37-48db-937e-58c612906982",
   "metadata": {},
   "source": [
    "## Convert data to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "69ef3de4-8f97-4e63-a3dd-c391b8a76780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'pl.parquet' already exists. Skipping...\n",
      "File 'hu.parquet' already exists. Skipping...\n",
      "File 'he.parquet' already exists. Skipping...\n",
      "File 'nl.parquet' already exists. Skipping...\n",
      "File 'ja.parquet' already exists. Skipping...\n",
      "File 'ru.parquet' already exists. Skipping...\n",
      "File 'it.parquet' already exists. Skipping...\n",
      "File 'en.parquet' already exists. Skipping...\n",
      "File 'es.parquet' already exists. Skipping...\n",
      "File 'pt.parquet' already exists. Skipping...\n",
      "File 'de.parquet' already exists. Skipping...\n",
      "File 'fr.parquet' already exists. Skipping...\n"
     ]
    }
   ],
   "source": [
    "# Path to the SQLite database file\n",
    "sqlite_file = 'wikibooks.sqlite'\n",
    "\n",
    "# Get the size of the SQLite database file\n",
    "sqlite_file_size_bytes = os.path.getsize(sqlite_file)\n",
    "# Convert the size from bytes to megabytes (MB)\n",
    "sqlite_file_size_mb = sqlite_file_size_bytes / (1024 ** 2)\n",
    "\n",
    "# Establish a connection to the SQLite database\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Execute the SQL query to retrieve table names\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "\n",
    "# Fetch all the table names\n",
    "table_names = cursor.fetchall()\n",
    "\n",
    "# Initialize a variable to hold the sum of the sizes of the Parquet files\n",
    "sum_parquet_files_size_bytes = 0\n",
    "\n",
    "# Iterate over the table names\n",
    "for table_name in table_names:\n",
    "    table_name = table_name[0]  # Extract the table name from the tuple\n",
    "\n",
    "    file_name = f\"{table_name}.parquet\"\n",
    "    \n",
    "    # Check if the Parquet file already exists\n",
    "    if os.path.exists(file_name):\n",
    "        print(f\"File '{file_name}' already exists. Skipping...\")\n",
    "        sum_parquet_files_size_bytes += os.path.getsize(file_name)\n",
    "        continue\n",
    "    \n",
    "    # Fetch all the data from the table\n",
    "    cursor.execute(f\"SELECT * FROM {table_name};\")\n",
    "    table_data = cursor.fetchall()\n",
    "\n",
    "    # Fetch the column names\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "    column_names = cursor.fetchall()\n",
    "    column_names = [column[1] for column in column_names]\n",
    "\n",
    "    # Create a pandas DataFrame from the fetched data\n",
    "    df = pd.DataFrame(table_data, columns=column_names)\n",
    "\n",
    "    # Convert to arrow format\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    # Save as Parquet file\n",
    "    pq.write_table(table, file_name, row_group_size=10000)\n",
    "\n",
    "    # Can also write df to parquet file directly but less flexible\n",
    "    # Save the DataFrame as a Parquet file\n",
    "    # df.to_parquet(file_name, index=False)\n",
    "    sum_parquet_files_size_bytes += os.path.getsize(file_name)\n",
    "\n",
    "    print(f\"Table '{table_name}' saved as '{file_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db9092-d0fc-46b2-8543-fc8990335572",
   "metadata": {},
   "source": [
    "## Space savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a09165ea-59cb-447a-9809-f5e3a0013ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of SQLite database file: 11701.34 MB\n",
      "Sum of sizes of Parquet files: 3309.74 MB\n",
      "Percentage of space saved by converting to Parquet: 71.71%\n"
     ]
    }
   ],
   "source": [
    "# Convert the sum of the sizes of the Parquet files from bytes to megabytes (MB)\n",
    "sum_parquet_files_size_mb = sum_parquet_files_size_bytes / (1024 ** 2)\n",
    "\n",
    "# Calculate the percentage of space saved\n",
    "space_savings_percentage = (1 - (sum_parquet_files_size_mb / sqlite_file_size_mb)) * 100\n",
    "\n",
    "# Print the size of the SQLite database file in MB\n",
    "print(f\"Size of SQLite database file: {sqlite_file_size_mb:.2f} MB\")\n",
    "\n",
    "# Print the sum of the sizes of the Parquet files in MB\n",
    "print(f\"Sum of sizes of Parquet files: {sum_parquet_files_size_mb:.2f} MB\")\n",
    "\n",
    "# Print the percentage of space saved\n",
    "print(f\"Percentage of space saved by converting to Parquet: {space_savings_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ff11ac-db8e-4321-a8bf-6a9c3fd6c711",
   "metadata": {},
   "source": [
    "# Inspecting Parquet Data Format\n",
    "\n",
    "As mentioned previously, parquet has a specific way of storing the columnar data that speeds up subsequent queries. One of the crucial aspects is the metadata for each column and organization of Column data into RowGroups. Here we will see how that looks like and what all statistics are pre-generated by parquet and stored along-side data to speed up queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe6d5997-1e22-414c-9957-5eb00d51d9e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.FileMetaData object at 0x7fbd288354e0>\n",
       "  created_by: parquet-cpp-arrow version 15.0.0\n",
       "  num_columns: 5\n",
       "  num_rows: 86736\n",
       "  num_row_groups: 9\n",
       "  format_version: 2.6\n",
       "  serialized_size: 21408"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the English dataset\n",
    "en_file =  \"en.parquet\"\n",
    "parquet_file = pq.ParquetFile(en_file)\n",
    "\n",
    "# Inspect file metadata\n",
    "metadata = parquet_file.metadata\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e603d478-5755-4219-ae16-3b837c6e5ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.ColumnChunkMetaData object at 0x7fbd042c7560>\n",
       "  file_offset: 325052\n",
       "  file_path: \n",
       "  physical_type: BYTE_ARRAY\n",
       "  num_values: 10000\n",
       "  path_in_schema: title\n",
       "  is_stats_set: True\n",
       "  statistics:\n",
       "    <pyarrow._parquet.Statistics object at 0x7fbd042c7650>\n",
       "      has_min_max: True\n",
       "      min: Wikibooks: .NET Development Foundation/AllInOne\n",
       "      max: Wikibooks: Þe ettbære Garden/S\n",
       "      null_count: 0\n",
       "      distinct_count: None\n",
       "      num_values: 10000\n",
       "      physical_type: BYTE_ARRAY\n",
       "      logical_type: String\n",
       "      converted_type (legacy): UTF8\n",
       "  compression: SNAPPY\n",
       "  encodings: ('PLAIN', 'RLE', 'RLE_DICTIONARY')\n",
       "  has_dictionary_page: True\n",
       "  dictionary_page_offset: 4\n",
       "  data_page_offset: 307407\n",
       "  total_compressed_size: 325048\n",
       "  total_uncompressed_size: 617999"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect metadata of a Row Group\n",
    "metadata.row_group(0).column(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c416e6-cc10-43dc-994d-1682059a9c30",
   "metadata": {},
   "source": [
    "- There are a total of 10000 rows in the RowGroup as expected because that is what we set when writing the parquet file.\n",
    "- Parquet is storing statistics for each column with min and max values, which is useful for eliminating row groups while reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c26f9ff-dc85-4826-a260-f167ba6bdd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9 RowGroups in en.parquet file\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {parquet_file.num_row_groups} RowGroups in {en_file} file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c807e87-d2f6-4018-8533-35c94da058d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.ParquetSchema object at 0x7fbd042aacc0>\n",
       "required group field_id=-1 schema {\n",
       "  optional binary field_id=-1 title (String);\n",
       "  optional binary field_id=-1 url (String);\n",
       "  optional binary field_id=-1 abstract (String);\n",
       "  optional binary field_id=-1 body_text (String);\n",
       "  optional binary field_id=-1 body_html (String);\n",
       "}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the data schema\n",
    "parquet_file.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8b022c56-4021-4de6-93fc-a0f4e2029f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "title: string\n",
      "url: string\n",
      "abstract: string\n",
      "body_text: string\n",
      "body_html: string\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_html</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikibooks: Social Statistics/Chapter 2</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Social_Statistic...</td>\n",
       "      <td>__NOTOC__</td>\n",
       "      <td>Linear Regression Models[edit | edit source]\\n...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;h1&gt;&lt;span class=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wikibooks: IB Chemistry</td>\n",
       "      <td>https://en.wikibooks.org/wiki/IB_Chemistry</td>\n",
       "      <td>__NOTOC__ __NOEDITSECTION__</td>\n",
       "      <td>Standard Level Chapters\\nThe last cohort of st...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;h2&gt;&lt;span class=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wikibooks: Field Guide/Animal Tracks/Raccoon</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Field_Guide/Anim...</td>\n",
       "      <td></td>\n",
       "      <td>Raccoon (Procyon lotor)\\n\\n\\n\\t\\t\\n\\t\\t\\t\\n\\t\\...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;div style=\"bord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wikibooks: Lua in SpringRTS/Variables and Cons...</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Lua_in_SpringRTS...</td>\n",
       "      <td>Here follows global constants and variables th...</td>\n",
       "      <td>Here follows global constants and variables th...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;p&gt;Here follows ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wikibooks: Solutions To Mathematics Textbooks/...</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Solutions_To_Mat...</td>\n",
       "      <td>=Chapter 6=</td>\n",
       "      <td>Contents\\n\\n1 Chapter 6\\n\\n1.1 7\\n\\n1.1.1 a\\n1...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;div id=\"toc\" cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Wikibooks: Radiation Oncology/RTOG Trials</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Radiation_Oncolo...</td>\n",
       "      <td>Overview of RTOG Trials</td>\n",
       "      <td>Front Page: Radiation Oncology | RTOG Trials |...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;table width=\"75...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Wikibooks: AP Biology/LABORATORY 11. Animal Be...</td>\n",
       "      <td>https://en.wikibooks.org/wiki/AP_Biology/LABOR...</td>\n",
       "      <td>This is a lab performed by AP Biology students...</td>\n",
       "      <td>This is a lab performed by AP Biology students...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;p&gt;This is a lab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Wikibooks: Web App Development with Google App...</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Web_App_Developm...</td>\n",
       "      <td>== Create a new script ==</td>\n",
       "      <td>Create a new script[edit | edit source]\\nYou c...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;h2&gt;&lt;span class=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Wikibooks: Artificial Intelligence/Search/Dijk...</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Artificial_Intel...</td>\n",
       "      <td>==Overview==</td>\n",
       "      <td>Contents\\n\\n1 Overview\\n2 Description of the A...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;div id=\"toc\" cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Wikibooks: Physics Exercises/Electrostatics</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Physics_Exercise...</td>\n",
       "      <td>===Electric field superposition principle   ===</td>\n",
       "      <td>Contents\\n\\n1 Electric field superposition pri...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;p&gt;&lt;br&gt;\\n&lt;/p&gt;\\n&lt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0                Wikibooks: Social Statistics/Chapter 2   \n",
       "1                               Wikibooks: IB Chemistry   \n",
       "2          Wikibooks: Field Guide/Animal Tracks/Raccoon   \n",
       "3     Wikibooks: Lua in SpringRTS/Variables and Cons...   \n",
       "4     Wikibooks: Solutions To Mathematics Textbooks/...   \n",
       "...                                                 ...   \n",
       "9995          Wikibooks: Radiation Oncology/RTOG Trials   \n",
       "9996  Wikibooks: AP Biology/LABORATORY 11. Animal Be...   \n",
       "9997  Wikibooks: Web App Development with Google App...   \n",
       "9998  Wikibooks: Artificial Intelligence/Search/Dijk...   \n",
       "9999        Wikibooks: Physics Exercises/Electrostatics   \n",
       "\n",
       "                                                    url  \\\n",
       "0     https://en.wikibooks.org/wiki/Social_Statistic...   \n",
       "1            https://en.wikibooks.org/wiki/IB_Chemistry   \n",
       "2     https://en.wikibooks.org/wiki/Field_Guide/Anim...   \n",
       "3     https://en.wikibooks.org/wiki/Lua_in_SpringRTS...   \n",
       "4     https://en.wikibooks.org/wiki/Solutions_To_Mat...   \n",
       "...                                                 ...   \n",
       "9995  https://en.wikibooks.org/wiki/Radiation_Oncolo...   \n",
       "9996  https://en.wikibooks.org/wiki/AP_Biology/LABOR...   \n",
       "9997  https://en.wikibooks.org/wiki/Web_App_Developm...   \n",
       "9998  https://en.wikibooks.org/wiki/Artificial_Intel...   \n",
       "9999  https://en.wikibooks.org/wiki/Physics_Exercise...   \n",
       "\n",
       "                                               abstract  \\\n",
       "0                                             __NOTOC__   \n",
       "1                           __NOTOC__ __NOEDITSECTION__   \n",
       "2                                                         \n",
       "3     Here follows global constants and variables th...   \n",
       "4                                           =Chapter 6=   \n",
       "...                                                 ...   \n",
       "9995                            Overview of RTOG Trials   \n",
       "9996  This is a lab performed by AP Biology students...   \n",
       "9997                          == Create a new script ==   \n",
       "9998                                       ==Overview==   \n",
       "9999    ===Electric field superposition principle   ===   \n",
       "\n",
       "                                              body_text  \\\n",
       "0     Linear Regression Models[edit | edit source]\\n...   \n",
       "1     Standard Level Chapters\\nThe last cohort of st...   \n",
       "2     Raccoon (Procyon lotor)\\n\\n\\n\\t\\t\\n\\t\\t\\t\\n\\t\\...   \n",
       "3     Here follows global constants and variables th...   \n",
       "4     Contents\\n\\n1 Chapter 6\\n\\n1.1 7\\n\\n1.1.1 a\\n1...   \n",
       "...                                                 ...   \n",
       "9995  Front Page: Radiation Oncology | RTOG Trials |...   \n",
       "9996  This is a lab performed by AP Biology students...   \n",
       "9997  Create a new script[edit | edit source]\\nYou c...   \n",
       "9998  Contents\\n\\n1 Overview\\n2 Description of the A...   \n",
       "9999  Contents\\n\\n1 Electric field superposition pri...   \n",
       "\n",
       "                                              body_html  \n",
       "0     <div class=\"mw-parser-output\"><h1><span class=...  \n",
       "1     <div class=\"mw-parser-output\"><h2><span class=...  \n",
       "2     <div class=\"mw-parser-output\"><div style=\"bord...  \n",
       "3     <div class=\"mw-parser-output\"><p>Here follows ...  \n",
       "4     <div class=\"mw-parser-output\"><div id=\"toc\" cl...  \n",
       "...                                                 ...  \n",
       "9995  <div class=\"mw-parser-output\"><table width=\"75...  \n",
       "9996  <div class=\"mw-parser-output\"><p>This is a lab...  \n",
       "9997  <div class=\"mw-parser-output\"><h2><span class=...  \n",
       "9998  <div class=\"mw-parser-output\"><div id=\"toc\" cl...  \n",
       "9999  <div class=\"mw-parser-output\"><p><br>\\n</p>\\n<...  \n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is possible to read invidual row groups, since RowGroup offsets are maintained by parquet file\n",
    "rg = parquet_file.read_row_group(3)\n",
    "print(rg.to_string())\n",
    "rg.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808e4c90-db4e-418e-b728-3e0acc8a7f8b",
   "metadata": {},
   "source": [
    "# Enrich Data\n",
    "We can add some useful metrics to make the dataset more interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad819f20-b791-4a1e-9e70-9fb70eba90a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parquet_file.read().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17a6241a-1488-475d-854b-5aa1137e8dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ydatta/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e72f3c9f-e68e-4095-adff-98389c1e84fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = set(stopwords.words('english'))\n",
    "lemma = nltk.WordNetLemmatizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(r'[^a-z]', ' ', text.lower())\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [lemma.lemmatize(w) for w in words if not w in stopwords_en]\n",
    "    return words\n",
    "def normalize(text):\n",
    "    return ' '.join(tokenize(text))\n",
    "def word_count(text):\n",
    "    return len(text.split(' '))\n",
    "def long_words(text, length):\n",
    "    return len([w for w in text.split(' ') if len(w) >= length])\n",
    "def max_word_len(text):\n",
    "    words = (re.sub(r\"[,.;@#?!&$]+\\ *-\", \" \", text)).split()\n",
    "    return np.max([len(w) for w in text.split(' ')])\n",
    "def avg_sen_len(text):\n",
    "    text.replace('...','.')\n",
    "    sen_lens = []\n",
    "    for sentence in text.split('.'):\n",
    "        sen_lens.append(len(sentence.split())) \n",
    "    return sum(sen_lens)/len(sen_lens)\n",
    "def avg_word_len(text):\n",
    "    words = (re.sub(r\"[,.;@#?!&$]+\\ *-\", \" \", text)).split()\n",
    "    return sum([len(word) for word in words])/len(words)\n",
    "def punct_count(text):\n",
    "    return sum(text.count(x) for x in \"[,.;:@#?!&$]+\\ *-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cd1a92e1-c112-43ab-ba5a-3be042e260bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_statistical_feature(df, text_col_name):\n",
    "    # df['tokens'] = [np.array(tokenize(str(df[text_col_name][i]))) for i in range(0, df[text_col_name].size)]\n",
    "    # df['normalized'] = [normalize(str(df[text_col_name][i])) for i in range(0, df[text_col_name].size)]\n",
    "    df['word_count'] = df[text_col_name].apply(word_count)\n",
    "    df['len'] = df[text_col_name].apply(len)\n",
    "    # df['punct_count'] = [punct_count(df[text_col_name][i]) for i in range(0, df[text_col_name].size)]\n",
    "    df['avg_sen_len'] = [avg_sen_len(df[text_col_name][i]) for i in range(0, df[text_col_name].size)]\n",
    "    # df['avg_word_len'] = [avg_word_len(df[text_col_name][i]) for i in range(0, df[text_col_name].size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4511dc9a-c3be-4fdb-b2fb-72cc58f44eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_statistical_feature(df, 'body_text')\n",
    "df['avg_sen_len'] = df['avg_sen_len'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "461ce759-c6c4-430c-865e-a6cb8cf80f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_html</th>\n",
       "      <th>word_count</th>\n",
       "      <th>len</th>\n",
       "      <th>avg_sen_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikibooks: Radiation Oncology/NHL/CLL-SLL</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Radiation_Oncolo...</td>\n",
       "      <td>Chronic Lymphocytic Leukemia and Small Lymphoc...</td>\n",
       "      <td>Front Page: Radiation Oncology | RTOG Trials |...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;table width=\"10...</td>\n",
       "      <td>620</td>\n",
       "      <td>4805</td>\n",
       "      <td>20.361111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wikibooks: Romanian/Lesson 9</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Romanian/Lesson_9</td>\n",
       "      <td>==Băuturi/Beverages==</td>\n",
       "      <td>Băuturi/Beverages[edit | edit source]\\nTea : C...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;h2&gt;&lt;span id=\"B....</td>\n",
       "      <td>84</td>\n",
       "      <td>827</td>\n",
       "      <td>30.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wikibooks: Karrigell</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Karrigell</td>\n",
       "      <td>Karrigell is an open Source Python web framewo...</td>\n",
       "      <td>Karrigell is an open Source Python web framewo...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;p&gt;Karrigell is ...</td>\n",
       "      <td>185</td>\n",
       "      <td>1250</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wikibooks: The Pyrogenesis Engine/0 A.D./GuiSe...</td>\n",
       "      <td>https://en.wikibooks.org/wiki/The_Pyrogenesis_...</td>\n",
       "      <td>====setupUnitPanel====</td>\n",
       "      <td>setupUnitPanel[edit | edit source]\\nHelper fun...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;h4&gt;&lt;span class=...</td>\n",
       "      <td>24</td>\n",
       "      <td>185</td>\n",
       "      <td>5.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wikibooks: LMIs in Control/pages/Exterior Coni...</td>\n",
       "      <td>https://en.wikibooks.org/wiki/LMIs_in_Control/...</td>\n",
       "      <td>== The Concept ==</td>\n",
       "      <td>Contents\\n\\n1 The Concept\\n2 The System\\n3 The...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;div id=\"toc\" cl...</td>\n",
       "      <td>7500</td>\n",
       "      <td>11040</td>\n",
       "      <td>16.236841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86731</th>\n",
       "      <td>Wikibooks: Python Programming/Creating Python ...</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Python_Programmi...</td>\n",
       "      <td>Welcome to Python! This tutorial will show you...</td>\n",
       "      <td>Previous: Self Help\\n\\nIndex\\n\\nNext: Variable...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;div class=\"nopr...</td>\n",
       "      <td>1008</td>\n",
       "      <td>6672</td>\n",
       "      <td>11.313131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86732</th>\n",
       "      <td>Wikibooks: Calculus/Precalculus</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Calculus/Precalc...</td>\n",
       "      <td>==Precalculus==</td>\n",
       "      <td>← Contributing\\n\\nCalculus\\n\\nAlgebra →\\n\\n\\nP...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;table width=\"10...</td>\n",
       "      <td>55</td>\n",
       "      <td>428</td>\n",
       "      <td>7.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86733</th>\n",
       "      <td>Wikibooks: Castles of England/Somerset</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Castles_of_Engla...</td>\n",
       "      <td>There are 11 castles in Somerset.</td>\n",
       "      <td>There are 11 castles in Somerset.\\n\\n\\n\\n\\nNam...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;p&gt;There are 11 ...</td>\n",
       "      <td>138</td>\n",
       "      <td>1646</td>\n",
       "      <td>12.529411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86734</th>\n",
       "      <td>Wikibooks: Digital Technology and Cultures/Int...</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Digital_Technolo...</td>\n",
       "      <td>=CULTURAL STUDIES AND IDENTITY=</td>\n",
       "      <td>Contents\\n\\n1 CULTURAL STUDIES AND IDENTITY\\n\\...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;div id=\"toc\" cl...</td>\n",
       "      <td>1266</td>\n",
       "      <td>8429</td>\n",
       "      <td>13.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86735</th>\n",
       "      <td>Wikibooks: English-Hanzi/Sardine is a nutritio...</td>\n",
       "      <td>https://en.wikibooks.org/wiki/English-Hanzi/Sa...</td>\n",
       "      <td>Sardine is a nutritious oily fish.</td>\n",
       "      <td>Sardine is a nutritious oily fish.\\n沙丁鱼是一种有营养的...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;p&gt;Sardine is a ...</td>\n",
       "      <td>6</td>\n",
       "      <td>49</td>\n",
       "      <td>3.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86736 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0              Wikibooks: Radiation Oncology/NHL/CLL-SLL   \n",
       "1                           Wikibooks: Romanian/Lesson 9   \n",
       "2                                   Wikibooks: Karrigell   \n",
       "3      Wikibooks: The Pyrogenesis Engine/0 A.D./GuiSe...   \n",
       "4      Wikibooks: LMIs in Control/pages/Exterior Coni...   \n",
       "...                                                  ...   \n",
       "86731  Wikibooks: Python Programming/Creating Python ...   \n",
       "86732                    Wikibooks: Calculus/Precalculus   \n",
       "86733             Wikibooks: Castles of England/Somerset   \n",
       "86734  Wikibooks: Digital Technology and Cultures/Int...   \n",
       "86735  Wikibooks: English-Hanzi/Sardine is a nutritio...   \n",
       "\n",
       "                                                     url  \\\n",
       "0      https://en.wikibooks.org/wiki/Radiation_Oncolo...   \n",
       "1        https://en.wikibooks.org/wiki/Romanian/Lesson_9   \n",
       "2                https://en.wikibooks.org/wiki/Karrigell   \n",
       "3      https://en.wikibooks.org/wiki/The_Pyrogenesis_...   \n",
       "4      https://en.wikibooks.org/wiki/LMIs_in_Control/...   \n",
       "...                                                  ...   \n",
       "86731  https://en.wikibooks.org/wiki/Python_Programmi...   \n",
       "86732  https://en.wikibooks.org/wiki/Calculus/Precalc...   \n",
       "86733  https://en.wikibooks.org/wiki/Castles_of_Engla...   \n",
       "86734  https://en.wikibooks.org/wiki/Digital_Technolo...   \n",
       "86735  https://en.wikibooks.org/wiki/English-Hanzi/Sa...   \n",
       "\n",
       "                                                abstract  \\\n",
       "0      Chronic Lymphocytic Leukemia and Small Lymphoc...   \n",
       "1                                  ==Băuturi/Beverages==   \n",
       "2      Karrigell is an open Source Python web framewo...   \n",
       "3                                 ====setupUnitPanel====   \n",
       "4                                      == The Concept ==   \n",
       "...                                                  ...   \n",
       "86731  Welcome to Python! This tutorial will show you...   \n",
       "86732                                    ==Precalculus==   \n",
       "86733                  There are 11 castles in Somerset.   \n",
       "86734                    =CULTURAL STUDIES AND IDENTITY=   \n",
       "86735                 Sardine is a nutritious oily fish.   \n",
       "\n",
       "                                               body_text  \\\n",
       "0      Front Page: Radiation Oncology | RTOG Trials |...   \n",
       "1      Băuturi/Beverages[edit | edit source]\\nTea : C...   \n",
       "2      Karrigell is an open Source Python web framewo...   \n",
       "3      setupUnitPanel[edit | edit source]\\nHelper fun...   \n",
       "4      Contents\\n\\n1 The Concept\\n2 The System\\n3 The...   \n",
       "...                                                  ...   \n",
       "86731  Previous: Self Help\\n\\nIndex\\n\\nNext: Variable...   \n",
       "86732  ← Contributing\\n\\nCalculus\\n\\nAlgebra →\\n\\n\\nP...   \n",
       "86733  There are 11 castles in Somerset.\\n\\n\\n\\n\\nNam...   \n",
       "86734  Contents\\n\\n1 CULTURAL STUDIES AND IDENTITY\\n\\...   \n",
       "86735  Sardine is a nutritious oily fish.\\n沙丁鱼是一种有营养的...   \n",
       "\n",
       "                                               body_html  word_count    len  \\\n",
       "0      <div class=\"mw-parser-output\"><table width=\"10...         620   4805   \n",
       "1      <div class=\"mw-parser-output\"><h2><span id=\"B....          84    827   \n",
       "2      <div class=\"mw-parser-output\"><p>Karrigell is ...         185   1250   \n",
       "3      <div class=\"mw-parser-output\"><h4><span class=...          24    185   \n",
       "4      <div class=\"mw-parser-output\"><div id=\"toc\" cl...        7500  11040   \n",
       "...                                                  ...         ...    ...   \n",
       "86731  <div class=\"mw-parser-output\"><div class=\"nopr...        1008   6672   \n",
       "86732  <div class=\"mw-parser-output\"><table width=\"10...          55    428   \n",
       "86733  <div class=\"mw-parser-output\"><p>There are 11 ...         138   1646   \n",
       "86734  <div class=\"mw-parser-output\"><div id=\"toc\" cl...        1266   8429   \n",
       "86735  <div class=\"mw-parser-output\"><p>Sardine is a ...           6     49   \n",
       "\n",
       "       avg_sen_len  \n",
       "0        20.361111  \n",
       "1        30.600000  \n",
       "2        24.000000  \n",
       "3         5.600000  \n",
       "4        16.236841  \n",
       "...            ...  \n",
       "86731    11.313131  \n",
       "86732     7.875000  \n",
       "86733    12.529411  \n",
       "86734    13.645833  \n",
       "86735     3.500000  \n",
       "\n",
       "[86736 rows x 8 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ddf5db46-3f4c-4719-98da-97b778a3d4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title           object\n",
      "url             object\n",
      "abstract        object\n",
      "body_text       object\n",
      "body_html       object\n",
      "word_count       int64\n",
      "len              int64\n",
      "avg_sen_len    float32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "665c9598-a504-407b-9a8b-010b111327a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save enriched dataset to parquet\n",
    "table = pa.Table.from_pandas(df)\n",
    "# Save as Parquet file\n",
    "pq.write_table(table, 'enriched_en.parquet', row_group_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81b893-ceda-44c2-8eda-d6ca0df04829",
   "metadata": {},
   "source": [
    "# Use pyspark to query data\n",
    "The query will leverage push down predicates to minimize IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3f4d0b17-c0d4-47ec-8f53-7453eeb36804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/22 22:54:00 WARN Utils: Your hostname, Radhe resolves to a loopback address: 127.0.1.1; using 192.168.1.109 instead (on interface wlp7s0)\n",
      "24/02/22 22:54:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/22 22:54:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"ParquetReader\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e73cd7f5-f94f-4ae6-be71-0df8795a146f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter (word_count#50L > 5000)\n",
      "+- Project [title#45, word_count#50L]\n",
      "   +- Relation [title#45,url#46,abstract#47,body_text#48,body_html#49,word_count#50L,len#51L,avg_sen_len#52] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "title: string, word_count: bigint\n",
      "Filter (word_count#50L > cast(5000 as bigint))\n",
      "+- Project [title#45, word_count#50L]\n",
      "   +- Relation [title#45,url#46,abstract#47,body_text#48,body_html#49,word_count#50L,len#51L,avg_sen_len#52] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [title#45, word_count#50L]\n",
      "+- Filter (isnotnull(word_count#50L) AND (word_count#50L > 5000))\n",
      "   +- Relation [title#45,url#46,abstract#47,body_text#48,body_html#49,word_count#50L,len#51L,avg_sen_len#52] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(word_count#50L) AND (word_count#50L > 5000))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [title#45,word_count#50L] Batched: true, DataFilters: [isnotnull(word_count#50L), (word_count#50L > 5000)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/ydatta/Workspace/de-experiments/data/parquet/enriched_en.pa..., PartitionFilters: [], PushedFilters: [IsNotNull(word_count), GreaterThan(word_count,5000)], ReadSchema: struct<title:string,word_count:bigint>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"enriched_en.parquet\")\n",
    "filtered_df = df.select('title', 'word_count').filter(df.word_count > 5000)\n",
    "\n",
    "# Print the physical plan, showing the predicate pushdown\n",
    "filtered_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0516e25-9491-4eee-9e9d-af965dac5b10",
   "metadata": {},
   "source": [
    "In the above physical plan, FileScan parquet shows Columnar Projection and Predicate Pushdown, something like:\n",
    "```\n",
    "FileScan parquet [title#45,word_count#50L] Batched: true, DataFilters: [isnotnull(word_count#50L), (word_count#50L > 5000)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "342d7ba3-b4ae-4de7-9fdf-233582333e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               title|word_count|\n",
      "+--------------------+----------+\n",
      "|Wikibooks: LMIs i...|      7500|\n",
      "|Wikibooks: Histor...|     45868|\n",
      "|Wikibooks: Neurol...|     11669|\n",
      "|Wikibooks: Linear...|     21238|\n",
      "|Wikibooks: Rieman...|      6576|\n",
      "|Wikibooks: LMIs i...|     17601|\n",
      "|Wikibooks: Femini...|      5245|\n",
      "|Wikibooks: Media ...|      7600|\n",
      "|Wikibooks: Fukush...|     13419|\n",
      "|Wikibooks: Engine...|      6655|\n",
      "|Wikibooks: Introd...|     15054|\n",
      "|Wikibooks: LMIs i...|     15717|\n",
      "|Wikibooks: Soil E...|      7814|\n",
      "|Wikibooks: Rock C...|      6662|\n",
      "|Wikibooks: LMIs i...|     18504|\n",
      "|Wikibooks: Pinyin...|     31424|\n",
      "|Wikibooks: Materi...|      5936|\n",
      "|Wikibooks: High S...|      9835|\n",
      "|Wikibooks: Apache...|     12714|\n",
      "|Wikibooks: Measur...|     25377|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0c2f26-7489-4e69-b94a-88342530d118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
