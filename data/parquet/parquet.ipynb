{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0e2f9c9-7f87-42f2-8c21-3d5d55738a37",
   "metadata": {},
   "source": [
    "# Parquet For Data Processing\n",
    "\n",
    "This is a demo of parquet data format and its capabilities for big data processing.\n",
    "For this demo, we will be using pandas, sqlite, pyarrow and pyspark libraries to demonstrate the parquet capabilities.\n",
    "The dataset that we will use is an sqlite dump of [wikibooks](https://www.kaggle.com/datasets/dhruvildave/wikibooks-dataset) from kaggle. It contains 270K chapters of wikibooks in 12 languages, but we will concentrate on the English version. To access this dataset you need to setup kaggle account and download your [kaggle.json file before proceeding](https://www.kaggle.com/docs/api#authentication).\n",
    "\n",
    "- pandas is an open source library providing high-performance, easy-to-use data structures and data analysis tools in Python.\n",
    "- sqlite is an embedded SQL database engine, that uses the more traditional [B-Tree data-format](https://www.sqlite.org/fileformat2.html) for storage on disk \n",
    "- pyarrow is Python API of the [Apache Arrow](https://arrow.apache.org/) framework that defines an in-memory data representation and can read/write parquet, including conversion to pandas. There are alternatives like [fastparquet](https://pypi.org/project/fastparquet/) which can also be explored.\n",
    "- pyspark is a Python API to the [Apache Spark Engine](https://spark.apache.org/), interfaces Python commands with a Java/Scala execution core, and thereby gives Python programmers access to the Parquet format as parquet is natively supported in spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66d1e0fa-16bb-4f1c-a500-5dd450ad1b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cf05d87-68a6-4793-b77b-24259b4217cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import FileUpload\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f19b96-c71b-4fc9-ba27-109ad86dd19c",
   "metadata": {},
   "source": [
    "# Creating Parquet dataset from sqlite\n",
    "We will fetch the sqlite dataset and convert and store each table into parquet files. We can then compare the on-disk sizes to get an idea of how efficient parquet is. Note that dataset is about 1.8G so might take a while to download depending on your network speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf57c6-e38b-4bb8-8d8e-ca0a53547cc9",
   "metadata": {},
   "source": [
    "## Setting up kaggle token and downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a4754c-5371-4103-b4e9-0ca2c5167863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle.json file not found. Please upload the file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6593e82f759f427290a077d58a8cd8f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='application/json', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download your kaggle credentials file from kaggle and supply it here, only necessary if you have not yet setup your kaggle credentials\n",
    "# in the .kaggle folder in your home dir\n",
    "# Path to the .kaggle directory\n",
    "kaggle_dir = os.path.expanduser('~/.kaggle')\n",
    "kaggle_file_path = os.path.join(kaggle_dir, 'kaggle.json')\n",
    "\n",
    "# Function to check and prompt for file upload\n",
    "def check_and_prompt_for_upload():\n",
    "    if not os.path.isfile(kaggle_file_path):\n",
    "        print(\"kaggle.json file not found. Please upload the file.\")\n",
    "        upload = FileUpload(accept='application/json', multiple=False)\n",
    "        display(upload)\n",
    "        return upload\n",
    "    else:\n",
    "        print(\"kaggle.json file already exists in the '~/.kaggle' directory.\")\n",
    "        return None\n",
    "\n",
    "# Adjusted function to process the uploaded file based on the provided structure and set permissions\n",
    "def process_uploaded_file(upload_widget):\n",
    "    # Ensure the .kaggle directory exists\n",
    "    os.makedirs(kaggle_dir, exist_ok=True)\n",
    "    \n",
    "    if upload_widget:\n",
    "        # Assuming the first item in the tuple is the file info dictionary\n",
    "        file_info = upload_widget.value[0]  # Extract the file details from the tuple\n",
    "        \n",
    "        content = file_info['content']\n",
    "        with open(kaggle_file_path, 'wb') as f:\n",
    "            f.write(content)\n",
    "        print(f\"'{file_info['name']}' has been moved to '{kaggle_dir}'.\")\n",
    "\n",
    "        # Set file permissions to 600\n",
    "        os.chmod(kaggle_file_path, 0o600)\n",
    "        print(f\"Permissions for '{file_info['name']}' set to 600.\")\n",
    "\n",
    "upload = check_and_prompt_for_upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e2f457-f36f-4947-bef7-deb43e030d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'kaggle.json' has been moved to '/home/ydatta/.kaggle'.\n",
      "Permissions for 'kaggle.json' set to 600.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if upload.value:\n",
    "        process_uploaded_file(upload)\n",
    "except NameError:\n",
    "    print(\"Upload widget not displayed or file already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d1898b0-192b-4ced-8051-26afdf75e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d03e29-7d7e-48dc-81a7-f3f1eb523d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading wikibooks-dataset.zip to /home/ydatta/Workspace/de-experiments/data/parquet\n",
      "100%|██████████████████████████████████████| 1.82G/1.82G [11:32<00:00, 3.35MB/s]\n",
      "100%|██████████████████████████████████████| 1.82G/1.82G [11:32<00:00, 2.83MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d dhruvildave/wikibooks-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24ced0d8-f657-4408-98a4-f7bebf3ccb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "file_name = 'wikibooks-dataset.zip' #the file is your dataset exact name\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "  zip.extractall()\n",
    "  print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ca91e-bf37-48db-937e-58c612906982",
   "metadata": {},
   "source": [
    "## Convert data to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69ef3de4-8f97-4e63-a3dd-c391b8a76780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'pl' saved as 'pl.parquet'\n",
      "Table 'hu' saved as 'hu.parquet'\n",
      "Table 'he' saved as 'he.parquet'\n",
      "Table 'nl' saved as 'nl.parquet'\n",
      "Table 'ja' saved as 'ja.parquet'\n",
      "Table 'ru' saved as 'ru.parquet'\n",
      "Table 'it' saved as 'it.parquet'\n",
      "Table 'en' saved as 'en.parquet'\n",
      "Table 'es' saved as 'es.parquet'\n",
      "Table 'pt' saved as 'pt.parquet'\n",
      "Table 'de' saved as 'de.parquet'\n",
      "Table 'fr' saved as 'fr.parquet'\n"
     ]
    }
   ],
   "source": [
    "# Path to the SQLite database file\n",
    "sqlite_file = 'wikibooks.sqlite'\n",
    "\n",
    "# Get the size of the SQLite database file\n",
    "sqlite_file_size_bytes = os.path.getsize(sqlite_file)\n",
    "# Convert the size from bytes to megabytes (MB)\n",
    "sqlite_file_size_mb = sqlite_file_size_bytes / (1024 ** 2)\n",
    "\n",
    "# Establish a connection to the SQLite database\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Execute the SQL query to retrieve table names\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "\n",
    "# Fetch all the table names\n",
    "table_names = cursor.fetchall()\n",
    "\n",
    "# Initialize a variable to hold the sum of the sizes of the Parquet files\n",
    "sum_parquet_files_size_bytes = 0\n",
    "\n",
    "# Iterate over the table names\n",
    "for table_name in table_names:\n",
    "    table_name = table_name[0]  # Extract the table name from the tuple\n",
    "\n",
    "    file_name = f\"{table_name}.parquet\"\n",
    "    \n",
    "    # Check if the Parquet file already exists\n",
    "    if os.path.exists(file_name):\n",
    "        print(f\"File '{file_name}' already exists. Skipping...\")\n",
    "        sum_parquet_files_size_bytes += os.path.getsize(file_name)\n",
    "        continue\n",
    "    \n",
    "    # Fetch all the data from the table\n",
    "    cursor.execute(f\"SELECT * FROM {table_name};\")\n",
    "    table_data = cursor.fetchall()\n",
    "\n",
    "    # Fetch the column names\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "    column_names = cursor.fetchall()\n",
    "    column_names = [column[1] for column in column_names]\n",
    "\n",
    "    # Create a pandas DataFrame from the fetched data\n",
    "    df = pd.DataFrame(table_data, columns=column_names)\n",
    "\n",
    "    # Convert to arrow format\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    # Save as Parquet file\n",
    "    pq.write_table(table, file_name, row_group_size=10000)\n",
    "\n",
    "    # Can also write df to parquet file directly but less flexible\n",
    "    # Save the DataFrame as a Parquet file\n",
    "    # df.to_parquet(file_name, index=False)\n",
    "    sum_parquet_files_size_bytes += os.path.getsize(file_name)\n",
    "\n",
    "    print(f\"Table '{table_name}' saved as '{file_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db9092-d0fc-46b2-8543-fc8990335572",
   "metadata": {},
   "source": [
    "## Space savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a09165ea-59cb-447a-9809-f5e3a0013ca0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sum_parquet_files_size_bytes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert the sum of the sizes of the Parquet files from bytes to megabytes (MB)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sum_parquet_files_size_mb \u001b[38;5;241m=\u001b[39m \u001b[43msum_parquet_files_size_bytes\u001b[49m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Calculate the percentage of space saved\u001b[39;00m\n\u001b[1;32m      5\u001b[0m space_savings_percentage \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m (sum_parquet_files_size_mb \u001b[38;5;241m/\u001b[39m sqlite_file_size_mb)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sum_parquet_files_size_bytes' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert the sum of the sizes of the Parquet files from bytes to megabytes (MB)\n",
    "sum_parquet_files_size_mb = sum_parquet_files_size_bytes / (1024 ** 2)\n",
    "\n",
    "# Calculate the percentage of space saved\n",
    "space_savings_percentage = (1 - (sum_parquet_files_size_mb / sqlite_file_size_mb)) * 100\n",
    "\n",
    "# Print the size of the SQLite database file in MB\n",
    "print(f\"Size of SQLite database file: {sqlite_file_size_mb:.2f} MB\")\n",
    "\n",
    "# Print the sum of the sizes of the Parquet files in MB\n",
    "print(f\"Sum of sizes of Parquet files: {sum_parquet_files_size_mb:.2f} MB\")\n",
    "\n",
    "# Print the percentage of space saved\n",
    "print(f\"Percentage of space saved by converting to Parquet: {space_savings_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ff11ac-db8e-4321-a8bf-6a9c3fd6c711",
   "metadata": {},
   "source": [
    "# Inspecting Parquet Data Format\n",
    "\n",
    "As mentioned previously, parquet has a specific way of storing the columnar data that speeds up subsequent queries. One of the crucial aspects is the metadata for each column and organization of Column data into RowGroups. Here we will see how that looks like and what all statistics are pre-generated by parquet and stored along-side data to speed up queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe6d5997-1e22-414c-9957-5eb00d51d9e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.FileMetaData object at 0x7f9ed57ce6b0>\n",
       "  created_by: parquet-cpp-arrow version 15.0.0\n",
       "  num_columns: 5\n",
       "  num_rows: 86736\n",
       "  num_row_groups: 9\n",
       "  format_version: 2.6\n",
       "  serialized_size: 21408"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the English dataset\n",
    "en_file =  \"en.parquet\"\n",
    "parquet_file = pq.ParquetFile(en_file)\n",
    "\n",
    "# Inspect file metadata\n",
    "metadata = parquet_file.metadata\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e603d478-5755-4219-ae16-3b837c6e5ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.ColumnChunkMetaData object at 0x7fbaf504cef0>\n",
       "  file_offset: 325052\n",
       "  file_path: \n",
       "  physical_type: BYTE_ARRAY\n",
       "  num_values: 10000\n",
       "  path_in_schema: title\n",
       "  is_stats_set: True\n",
       "  statistics:\n",
       "    <pyarrow._parquet.Statistics object at 0x7fbaf504c810>\n",
       "      has_min_max: True\n",
       "      min: Wikibooks: .NET Development Foundation/AllInOne\n",
       "      max: Wikibooks: Þe ettbære Garden/S\n",
       "      null_count: 0\n",
       "      distinct_count: None\n",
       "      num_values: 10000\n",
       "      physical_type: BYTE_ARRAY\n",
       "      logical_type: String\n",
       "      converted_type (legacy): UTF8\n",
       "  compression: SNAPPY\n",
       "  encodings: ('PLAIN', 'RLE', 'RLE_DICTIONARY')\n",
       "  has_dictionary_page: True\n",
       "  dictionary_page_offset: 4\n",
       "  data_page_offset: 307407\n",
       "  total_compressed_size: 325048\n",
       "  total_uncompressed_size: 617999"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect metadata of a Row Group\n",
    "metadata.row_group(0).column(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c416e6-cc10-43dc-994d-1682059a9c30",
   "metadata": {},
   "source": [
    "- There are a total of 10000 rows in the RowGroup as expected because that is what we set when writing the parquet file.\n",
    "- Parquet is storing statistics for each column with min and max values, which is useful for eliminating row groups while reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c26f9ff-dc85-4826-a260-f167ba6bdd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {parquet_file.num_row_groups} RowGroups in {en_file} file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c807e87-d2f6-4018-8533-35c94da058d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.ParquetSchema object at 0x7fbac62db340>\n",
       "required group field_id=-1 schema {\n",
       "  optional binary field_id=-1 title (String);\n",
       "  optional binary field_id=-1 url (String);\n",
       "  optional binary field_id=-1 abstract (String);\n",
       "  optional binary field_id=-1 body_text (String);\n",
       "  optional binary field_id=-1 body_html (String);\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the data schema\n",
    "parquet_file.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b022c56-4021-4de6-93fc-a0f4e2029f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "title: string\n",
      "url: string\n",
      "abstract: string\n",
      "body_text: string\n",
      "body_html: string\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_html</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikibooks: Social Statistics/Chapter 2</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Social_Statistic...</td>\n",
       "      <td>__NOTOC__</td>\n",
       "      <td>Linear Regression Models[edit | edit source]\\n...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;h1&gt;&lt;span class=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wikibooks: IB Chemistry</td>\n",
       "      <td>https://en.wikibooks.org/wiki/IB_Chemistry</td>\n",
       "      <td>__NOTOC__ __NOEDITSECTION__</td>\n",
       "      <td>Standard Level Chapters\\nThe last cohort of st...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;h2&gt;&lt;span class=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wikibooks: Field Guide/Animal Tracks/Raccoon</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Field_Guide/Anim...</td>\n",
       "      <td></td>\n",
       "      <td>Raccoon (Procyon lotor)\\n\\n\\n\\t\\t\\n\\t\\t\\t\\n\\t\\...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;div style=\"bord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wikibooks: Lua in SpringRTS/Variables and Cons...</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Lua_in_SpringRTS...</td>\n",
       "      <td>Here follows global constants and variables th...</td>\n",
       "      <td>Here follows global constants and variables th...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;p&gt;Here follows ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wikibooks: Solutions To Mathematics Textbooks/...</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Solutions_To_Mat...</td>\n",
       "      <td>=Chapter 6=</td>\n",
       "      <td>Contents\\n\\n1 Chapter 6\\n\\n1.1 7\\n\\n1.1.1 a\\n1...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;div id=\"toc\" cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Wikibooks: Radiation Oncology/RTOG Trials</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Radiation_Oncolo...</td>\n",
       "      <td>Overview of RTOG Trials</td>\n",
       "      <td>Front Page: Radiation Oncology | RTOG Trials |...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;table width=\"75...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Wikibooks: AP Biology/LABORATORY 11. Animal Be...</td>\n",
       "      <td>https://en.wikibooks.org/wiki/AP_Biology/LABOR...</td>\n",
       "      <td>This is a lab performed by AP Biology students...</td>\n",
       "      <td>This is a lab performed by AP Biology students...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;p&gt;This is a lab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Wikibooks: Web App Development with Google App...</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Web_App_Developm...</td>\n",
       "      <td>== Create a new script ==</td>\n",
       "      <td>Create a new script[edit | edit source]\\nYou c...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;h2&gt;&lt;span class=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Wikibooks: Artificial Intelligence/Search/Dijk...</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Artificial_Intel...</td>\n",
       "      <td>==Overview==</td>\n",
       "      <td>Contents\\n\\n1 Overview\\n2 Description of the A...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;div id=\"toc\" cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Wikibooks: Physics Exercises/Electrostatics</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Physics_Exercise...</td>\n",
       "      <td>===Electric field superposition principle   ===</td>\n",
       "      <td>Contents\\n\\n1 Electric field superposition pri...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;p&gt;&lt;br&gt;\\n&lt;/p&gt;\\n&lt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0                Wikibooks: Social Statistics/Chapter 2   \n",
       "1                               Wikibooks: IB Chemistry   \n",
       "2          Wikibooks: Field Guide/Animal Tracks/Raccoon   \n",
       "3     Wikibooks: Lua in SpringRTS/Variables and Cons...   \n",
       "4     Wikibooks: Solutions To Mathematics Textbooks/...   \n",
       "...                                                 ...   \n",
       "9995          Wikibooks: Radiation Oncology/RTOG Trials   \n",
       "9996  Wikibooks: AP Biology/LABORATORY 11. Animal Be...   \n",
       "9997  Wikibooks: Web App Development with Google App...   \n",
       "9998  Wikibooks: Artificial Intelligence/Search/Dijk...   \n",
       "9999        Wikibooks: Physics Exercises/Electrostatics   \n",
       "\n",
       "                                                    url  \\\n",
       "0     https://en.wikibooks.org/wiki/Social_Statistic...   \n",
       "1            https://en.wikibooks.org/wiki/IB_Chemistry   \n",
       "2     https://en.wikibooks.org/wiki/Field_Guide/Anim...   \n",
       "3     https://en.wikibooks.org/wiki/Lua_in_SpringRTS...   \n",
       "4     https://en.wikibooks.org/wiki/Solutions_To_Mat...   \n",
       "...                                                 ...   \n",
       "9995  https://en.wikibooks.org/wiki/Radiation_Oncolo...   \n",
       "9996  https://en.wikibooks.org/wiki/AP_Biology/LABOR...   \n",
       "9997  https://en.wikibooks.org/wiki/Web_App_Developm...   \n",
       "9998  https://en.wikibooks.org/wiki/Artificial_Intel...   \n",
       "9999  https://en.wikibooks.org/wiki/Physics_Exercise...   \n",
       "\n",
       "                                               abstract  \\\n",
       "0                                             __NOTOC__   \n",
       "1                           __NOTOC__ __NOEDITSECTION__   \n",
       "2                                                         \n",
       "3     Here follows global constants and variables th...   \n",
       "4                                           =Chapter 6=   \n",
       "...                                                 ...   \n",
       "9995                            Overview of RTOG Trials   \n",
       "9996  This is a lab performed by AP Biology students...   \n",
       "9997                          == Create a new script ==   \n",
       "9998                                       ==Overview==   \n",
       "9999    ===Electric field superposition principle   ===   \n",
       "\n",
       "                                              body_text  \\\n",
       "0     Linear Regression Models[edit | edit source]\\n...   \n",
       "1     Standard Level Chapters\\nThe last cohort of st...   \n",
       "2     Raccoon (Procyon lotor)\\n\\n\\n\\t\\t\\n\\t\\t\\t\\n\\t\\...   \n",
       "3     Here follows global constants and variables th...   \n",
       "4     Contents\\n\\n1 Chapter 6\\n\\n1.1 7\\n\\n1.1.1 a\\n1...   \n",
       "...                                                 ...   \n",
       "9995  Front Page: Radiation Oncology | RTOG Trials |...   \n",
       "9996  This is a lab performed by AP Biology students...   \n",
       "9997  Create a new script[edit | edit source]\\nYou c...   \n",
       "9998  Contents\\n\\n1 Overview\\n2 Description of the A...   \n",
       "9999  Contents\\n\\n1 Electric field superposition pri...   \n",
       "\n",
       "                                              body_html  \n",
       "0     <div class=\"mw-parser-output\"><h1><span class=...  \n",
       "1     <div class=\"mw-parser-output\"><h2><span class=...  \n",
       "2     <div class=\"mw-parser-output\"><div style=\"bord...  \n",
       "3     <div class=\"mw-parser-output\"><p>Here follows ...  \n",
       "4     <div class=\"mw-parser-output\"><div id=\"toc\" cl...  \n",
       "...                                                 ...  \n",
       "9995  <div class=\"mw-parser-output\"><table width=\"75...  \n",
       "9996  <div class=\"mw-parser-output\"><p>This is a lab...  \n",
       "9997  <div class=\"mw-parser-output\"><h2><span class=...  \n",
       "9998  <div class=\"mw-parser-output\"><div id=\"toc\" cl...  \n",
       "9999  <div class=\"mw-parser-output\"><p><br>\\n</p>\\n<...  \n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is possible to read invidual row groups, since RowGroup offsets are maintained by parquet file\n",
    "rg = parquet_file.read_row_group(3)\n",
    "print(rg.to_string())\n",
    "rg.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffb700c-7224-4a33-ad5d-f757eaeb5e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
