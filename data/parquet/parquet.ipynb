{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0e2f9c9-7f87-42f2-8c21-3d5d55738a37",
   "metadata": {},
   "source": [
    "# Parquet For Data Processing\n",
    "\n",
    "This is a demo of parquet data format and its capabilities for big data processing.\n",
    "For this demo, we will be using pandas, sqlite, pyarrow and pyspark libraries to demonstrate the parquet capabilities.\n",
    "The dataset that we will use is an sqlite dump of [wikibooks](https://www.kaggle.com/datasets/dhruvildave/wikibooks-dataset) from kaggle. It contains 270K chapters of wikibooks in 12 languages, but we will concentrate on the English version. To access this dataset you need to setup kaggle account and download your [kaggle.json file before proceeding](https://www.kaggle.com/docs/api#authentication).\n",
    "\n",
    "- pandas is an open source library providing high-performance, easy-to-use data structures and data analysis tools in Python.\n",
    "- sqlite is an embedded SQL database engine, that uses the more traditional [B-Tree data-format](https://www.sqlite.org/fileformat2.html) for storage on disk \n",
    "- pyarrow is Python API of the [Apache Arrow](https://arrow.apache.org/) framework that defines an in-memory data representation and can read/write parquet, including conversion to pandas.\n",
    "- pyspark is a Python API to the [Apache Spark Engine](https://spark.apache.org/), interfaces Python commands with a Java/Scala execution core, and thereby gives Python programmers access to the Parquet format as parquet is natively supported in spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66d1e0fa-16bb-4f1c-a500-5dd450ad1b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cf05d87-68a6-4793-b77b-24259b4217cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import FileUpload\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f19b96-c71b-4fc9-ba27-109ad86dd19c",
   "metadata": {},
   "source": [
    "# Creating Parquet dataset from sqlite\n",
    "We will fetch the sqlite dataset and convert and store each table into parquet files. We can then compare the on-disk sizes to get an idea of how efficient parquet is. Note that dataset is about 1.8G so might take a while to download depending on your network speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf57c6-e38b-4bb8-8d8e-ca0a53547cc9",
   "metadata": {},
   "source": [
    "## Setting up kaggle token and downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7a4754c-5371-4103-b4e9-0ca2c5167863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle.json file not found. Please upload the file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9da1d0e828a4e1ca1b1ee7569effda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='application/json', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download your kaggle credentials file from kaggle and supply it here, only necessary if you have not yet setup your kaggle credentials\n",
    "# in the .kaggle folder in your home dir\n",
    "# Path to the .kaggle directory\n",
    "kaggle_dir = os.path.expanduser('~/.kaggle')\n",
    "kaggle_file_path = os.path.join(kaggle_dir, 'kaggle.json')\n",
    "\n",
    "# Function to check and prompt for file upload\n",
    "def check_and_prompt_for_upload():\n",
    "    if not os.path.isfile(kaggle_file_path):\n",
    "        print(\"kaggle.json file not found. Please upload the file.\")\n",
    "        upload = FileUpload(accept='application/json', multiple=False)\n",
    "        display(upload)\n",
    "        return upload\n",
    "    else:\n",
    "        print(\"kaggle.json file already exists in the '~/.kaggle' directory.\")\n",
    "        return None\n",
    "\n",
    "# Adjusted function to process the uploaded file based on the provided structure and set permissions\n",
    "def process_uploaded_file(upload_widget):\n",
    "    # Ensure the .kaggle directory exists\n",
    "    os.makedirs(kaggle_dir, exist_ok=True)\n",
    "    \n",
    "    if upload_widget:\n",
    "        # Assuming the first item in the tuple is the file info dictionary\n",
    "        file_info = upload_widget.value[0]  # Extract the file details from the tuple\n",
    "        \n",
    "        content = file_info['content']\n",
    "        with open(kaggle_file_path, 'wb') as f:\n",
    "            f.write(content)\n",
    "        print(f\"'{file_info['name']}' has been moved to '{kaggle_dir}'.\")\n",
    "\n",
    "        # Set file permissions to 600\n",
    "        os.chmod(kaggle_file_path, 0o600)\n",
    "        print(f\"Permissions for '{file_info['name']}' set to 600.\")\n",
    "\n",
    "upload = check_and_prompt_for_upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28e2f457-f36f-4947-bef7-deb43e030d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'kaggle.json' has been moved to '/Users/yashdatta/.kaggle'.\n",
      "Permissions for 'kaggle.json' set to 600.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if upload.value:\n",
    "        process_uploaded_file(upload)\n",
    "except NameError:\n",
    "    print(\"Upload widget not displayed or file already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d1898b0-192b-4ced-8051-26afdf75e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9d03e29-7d7e-48dc-81a7-f3f1eb523d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/yashdatta/.kaggle/kaggle.json'\n",
      "Downloading wikibooks-dataset.zip to /Users/yashdatta/Documents/Workspace/de-experiments/data/parquet\n",
      "100%|██████████████████████████████████████| 1.82G/1.82G [01:41<00:00, 18.9MB/s]\n",
      "100%|██████████████████████████████████████| 1.82G/1.82G [01:41<00:00, 19.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d dhruvildave/wikibooks-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24ced0d8-f657-4408-98a4-f7bebf3ccb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "file_name = 'wikibooks-dataset.zip' #the file is your dataset exact name\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "  zip.extractall()\n",
    "  print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ca91e-bf37-48db-937e-58c612906982",
   "metadata": {},
   "source": [
    "## Convert data to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69ef3de4-8f97-4e63-a3dd-c391b8a76780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'pl.parquet' already exists. Skipping...\n",
      "File 'hu.parquet' already exists. Skipping...\n",
      "File 'he.parquet' already exists. Skipping...\n",
      "File 'nl.parquet' already exists. Skipping...\n",
      "File 'ja.parquet' already exists. Skipping...\n",
      "File 'ru.parquet' already exists. Skipping...\n",
      "File 'it.parquet' already exists. Skipping...\n",
      "File 'en.parquet' already exists. Skipping...\n",
      "File 'es.parquet' already exists. Skipping...\n",
      "File 'pt.parquet' already exists. Skipping...\n",
      "File 'de.parquet' already exists. Skipping...\n",
      "File 'fr.parquet' already exists. Skipping...\n"
     ]
    }
   ],
   "source": [
    "# Path to the SQLite database file\n",
    "sqlite_file = 'wikibooks.sqlite'\n",
    "\n",
    "# Get the size of the SQLite database file\n",
    "sqlite_file_size_bytes = os.path.getsize(sqlite_file)\n",
    "# Convert the size from bytes to megabytes (MB)\n",
    "sqlite_file_size_mb = sqlite_file_size_bytes / (1024 ** 2)\n",
    "\n",
    "# Establish a connection to the SQLite database\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Execute the SQL query to retrieve table names\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "\n",
    "# Fetch all the table names\n",
    "table_names = cursor.fetchall()\n",
    "\n",
    "# Initialize a variable to hold the sum of the sizes of the Parquet files\n",
    "sum_parquet_files_size_bytes = 0\n",
    "\n",
    "# Iterate over the table names\n",
    "for table_name in table_names:\n",
    "    table_name = table_name[0]  # Extract the table name from the tuple\n",
    "\n",
    "    file_name = f\"{table_name}.parquet\"\n",
    "    \n",
    "    # Check if the Parquet file already exists\n",
    "    if os.path.exists(file_name):\n",
    "        print(f\"File '{file_name}' already exists. Skipping...\")\n",
    "        sum_parquet_files_size_bytes += os.path.getsize(file_name)\n",
    "        continue\n",
    "    \n",
    "    # Fetch all the data from the table\n",
    "    cursor.execute(f\"SELECT * FROM {table_name};\")\n",
    "    table_data = cursor.fetchall()\n",
    "\n",
    "    # Fetch the column names\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "    column_names = cursor.fetchall()\n",
    "    column_names = [column[1] for column in column_names]\n",
    "\n",
    "    # Create a pandas DataFrame from the fetched data\n",
    "    df = pd.DataFrame(table_data, columns=column_names)\n",
    "\n",
    "    # Save the DataFrame as a Parquet file\n",
    "    df.to_parquet(file_name, index=False)\n",
    "    sum_parquet_files_size_bytes += os.path.getsize(file_name)\n",
    "\n",
    "    print(f\"Table '{table_name}' saved as '{file_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db9092-d0fc-46b2-8543-fc8990335572",
   "metadata": {},
   "source": [
    "## Space savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a09165ea-59cb-447a-9809-f5e3a0013ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of SQLite database file: 11701.34 MB\n",
      "Sum of sizes of Parquet files: 3309.33 MB\n",
      "Percentage of space saved by converting to Parquet: 71.72%\n"
     ]
    }
   ],
   "source": [
    "# Convert the sum of the sizes of the Parquet files from bytes to megabytes (MB)\n",
    "sum_parquet_files_size_mb = sum_parquet_files_size_bytes / (1024 ** 2)\n",
    "\n",
    "# Calculate the percentage of space saved\n",
    "space_savings_percentage = (1 - (sum_parquet_files_size_mb / sqlite_file_size_mb)) * 100\n",
    "\n",
    "# Print the size of the SQLite database file in MB\n",
    "print(f\"Size of SQLite database file: {sqlite_file_size_mb:.2f} MB\")\n",
    "\n",
    "# Print the sum of the sizes of the Parquet files in MB\n",
    "print(f\"Sum of sizes of Parquet files: {sum_parquet_files_size_mb:.2f} MB\")\n",
    "\n",
    "# Print the percentage of space saved\n",
    "print(f\"Percentage of space saved by converting to Parquet: {space_savings_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ff11ac-db8e-4321-a8bf-6a9c3fd6c711",
   "metadata": {},
   "source": [
    "# Inspecting Parquet Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf2fdc8-2b87-4476-9002-63a2811263d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
